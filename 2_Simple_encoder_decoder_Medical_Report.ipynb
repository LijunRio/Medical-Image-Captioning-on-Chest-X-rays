{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Simple_encoder_decoder_Medical_Report.ipynb",
      "provenance": [],
      "mount_file_id": "1gMUkLlBDy2TJk-NpH7n4QchcvjYVFaT6",
      "authorship_tag": "ABX9TyNtzYE7Lqj4FfVvsU0h+8cx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishthomaschempolil/Medical-Image-Captioning-on-Chest-X-rays/blob/main/2_Simple_encoder_decoder_Medical_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp78hvbnKJS5"
      },
      "source": [
        "image_folder = '/content/drive/My Drive/Medical image Reporting/Images'\n",
        "df_path = '/content/drive/My Drive/Medical image Reporting/df_final.pkl'\n",
        "chexnet_weights = '/content/drive/My Drive/Medical image Reporting/ChexNet weights/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HCj-KNqJ0MF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib #for saving model files as pkl\n",
        "import os\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import imgaug.augmenters as iaa\n",
        "sns.set(palette='muted',style='white')\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D, Input, Embedding, LSTM,Dot,Reshape,Concatenate,BatchNormalization, GlobalMaxPooling2D, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMKUtqyupMHb"
      },
      "source": [
        "# **Creating Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trX7yvkgpRYW",
        "outputId": "4066213f-db29-484d-a416-75696375c3f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "df = pd.read_pickle(df_path)\n",
        "col = ['image_1','image_2','impression']\n",
        "df = df[col].copy()\n",
        "#path\n",
        "df['image_1'] = df['image_1'].apply(lambda row: os.path.join(image_folder,row)) #https://stackoverflow.com/a/61880790\n",
        "df['image_2'] = df['image_2'].apply(lambda row: os.path.join(image_folder,row))\n",
        "\n",
        "df.impression = '<CLS> ' + df.impression + ' <END>' \n",
        "print(df.shape)\n",
        "df.head(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4033, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_1</th>\n",
              "      <th>image_2</th>\n",
              "      <th>impression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>&lt;CLS&gt; bilateral lower lobe opacities . the app...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>&lt;CLS&gt; bilateral lower lung airspace disease ri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             image_1  ...                                         impression\n",
              "0  /content/drive/My Drive/Medical image Reportin...  ...  <CLS> bilateral lower lobe opacities . the app...\n",
              "1  /content/drive/My Drive/Medical image Reportin...  ...  <CLS> bilateral lower lung airspace disease ri...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkRsmY0TuMah",
        "outputId": "60ef9525-099e-458d-d8ab-66196d22513d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df.sample(random_state = 420,frac = 1).reset_index(drop = True) #shuffling the dataframe\n",
        "\n",
        "#splitting to train,test with 0.9,0.1 ratio\n",
        "train_size = int(0.9*df.shape[0])\n",
        "train = df[:train_size]\n",
        "test = df[train_size:].reset_index()\n",
        "\n",
        "del df,train_size\n",
        "train.shape,test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3629, 3), (404, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgjFwjEjpSFa",
        "outputId": "f79d3269-e254-45f1-f375-2a15e3b2b7fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "source": [
        "#tokenizer\n",
        "tokenizer = Tokenizer(filters = '',oov_token = '<unk>') #setting filters to none\n",
        "tokenizer.fit_on_texts(train.impression.values)\n",
        "train_captions = tokenizer.texts_to_sequences(train.impression) \n",
        "test_captions = tokenizer.texts_to_sequences(test.impression) \n",
        "vocab_size = len(tokenizer.word_index)\n",
        "caption_len = np.array([len(i) for i in train_captions])\n",
        "start_index = tokenizer.word_index['<cls>'] #tokened value of <cls>\n",
        "end_index = tokenizer.word_index['<end>'] #tokened value of <end>\n",
        "\n",
        "#visualising impression length and other details\n",
        "ax = sns.displot(caption_len,height = 6)\n",
        "ax.set_titles('Value Counts vs Caption Length')\n",
        "ax.set_xlabels('Impresion length')\n",
        "plt.show()\n",
        "print('\\nValue Counts for caption length top 5 values\\n')\n",
        "print('Length|Counts')\n",
        "print(pd.Series(caption_len).value_counts()[:5])\n",
        "print('\\nThe max and min value of \"caption length\" was found to be %i and %i respectively'%(max(caption_len),min(caption_len)))\n",
        "print('The 99.5 percentile value of caption_len which is %i will be taken as the maximum padded value for each impression'%(np.percentile(caption_len,99.5)))\n",
        "max_pad = int(np.percentile(caption_len,99.5))\n",
        "del train_captions,test_captions #we will create tokenizing  and padding in-built in dataloader"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Value Counts for caption length top 5 values\n",
            "\n",
            "Length|Counts\n",
            "7     1111\n",
            "6      409\n",
            "8      342\n",
            "12     134\n",
            "9      122\n",
            "dtype: int64\n",
            "\n",
            "The max and min value of \"caption length\" was found to be 135 and 3 respectively\n",
            "The 99.5 percentile value of caption_len which is 74 will be taken as the maximum padded value for each impression\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8Y1fR8RIxWB"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju4j3gOoLBpJ"
      },
      "source": [
        "## **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WojEEVYsLErk"
      },
      "source": [
        "batch_size = 2\n",
        "embedding_dim = 300\n",
        "dense_dim = 256\n",
        "lstm_units = dense_dim+embedding_dim\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R58RYZR904HU"
      },
      "source": [
        "## **Creating an input data pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZWYibRUYWGu"
      },
      "source": [
        "class Dataset():\n",
        "  #here we will get the images converted to vector form and the corresponding captions\n",
        "  def __init__(self,df,input_size,tokenizer = tokenizer, augmentation = True,max_pad = max_pad): \n",
        "    \"\"\"\n",
        "    df  = dataframe containing image_1,image_2 and impression\n",
        "    \"\"\"\n",
        "    self.image1 = df.image_1\n",
        "    self.image2 = df.image_2\n",
        "    self.caption = df.impression\n",
        "    self.input_size = input_size #tuple ex: (512,512)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.augmentation = augmentation\n",
        "    self.max_pad = max_pad\n",
        "\n",
        "    #image augmentation\n",
        "    #https://imgaug.readthedocs.io/en/latest/source/overview/flip.html?highlight=Fliplr\n",
        "    self.aug1 = iaa.Fliplr(1) #flip images horizaontally\n",
        "    self.aug2 = iaa.Flipud(1) #flip images vertically\n",
        "\n",
        "    # https://imgaug.readthedocs.io/en/latest/source/overview/convolutional.html?highlight=emboss#emboss\n",
        "    self.aug3 = iaa.Emboss(alpha=(1), strength=1) #embosses image\n",
        "\n",
        "    #https://imgaug.readthedocs.io/en/latest/source/api_augmenters_convolutional.html?highlight=sharpen#imgaug.augmenters.convolutional.Sharpen\n",
        "    self.aug4 = iaa.Sharpen(alpha=(1.0), lightness=(1.5)) #sharpens the image and apply some lightness/brighteness 1 means fully sharpened etc\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    #gets the datapoint at i th index, we will extract the feature vectors of images after resizing the image to  and apply augmentation\n",
        "    #from image segmentation assignment\n",
        "    image1 = cv2.imread(self.image1[i],cv2.IMREAD_UNCHANGED)/255 \n",
        "    image2 = cv2.imread(self.image2[i],cv2.IMREAD_UNCHANGED)/255 #here there are 3 channels\n",
        "    image1 = cv2.resize(image1,self.input_size,interpolation = cv2.INTER_NEAREST)\n",
        "    image2 = cv2.resize(image2,self.input_size,interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "    #tokenizing and padding\n",
        "    caption = self.tokenizer.texts_to_sequences(self.caption[i:i+1]) #the input should be an array for tokenizer ie [self.caption[i]] \n",
        "\n",
        "    caption = pad_sequences(caption,maxlen = self.max_pad,padding = 'post') #opshape: batch_size*1*input_length\n",
        "    caption = tf.squeeze(caption,axis=0)\n",
        "\n",
        "\n",
        "    if self.augmentation: #we will not apply augmentation that crops the image \n",
        "          a = np.random.uniform()\n",
        "          if a<0.2:\n",
        "              image1 = self.aug1.augment_image(image1)\n",
        "              image2 = self.aug1.augment_image(image2)\n",
        "          elif a<0.4:\n",
        "              image1 = self.aug2.augment_image(image1)\n",
        "              image2 = self.aug2.augment_image(image2)\n",
        "          elif a<0.6:\n",
        "              image1 = self.aug3.augment_image(image1)\n",
        "              image2 = self.aug3.augment_image(image2)\n",
        "          elif a<0.8:\n",
        "              image1 = self.aug4.augment_image(image1)\n",
        "              image2 = self.aug4.augment_image(image2)\n",
        "          else: #applying no augmentation\n",
        "            pass;\n",
        "\n",
        "    \n",
        "    return image1,image2,caption\n",
        "\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.image1)\n",
        "\n",
        "\n",
        "class Dataloader(tf.keras.utils.Sequence):     #for batching\n",
        "    def __init__(self, dataset, batch_size=1, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.dataset[0]))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = [self.dataset[j] for j in range(start,stop)] #taken from Data class (calls __getitem__ of Data) here the shape is batch_size*3, (image_1,image_2,caption)\n",
        "        \n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)] #here the shape will become batch_size*input_size(of image)*3,batch_size*input_size(of image)*3\n",
        "                                                                      #,batch_size*1*max_pad\n",
        "\n",
        "\n",
        "        return tuple([[batch[0],batch[1],batch[2]],batch[2][:,1:]]) #here [image1,image2, caption(without <END>)],caption(without <CLS>)\n",
        "    \n",
        "    def __len__(self): #returns total number of batches in an epoch\n",
        "        return len(self.indexes) // self.batch_size\n",
        "    \n",
        "    def on_epoch_end(self): #it runs at the end of epoch\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh3MLkj7ofJq"
      },
      "source": [
        "input_size = (224,224)\n",
        "train_dataloader = Dataset(train,input_size)\n",
        "train_dataloader = Dataloader(train_dataloader,batch_size = batch_size)\n",
        "\n",
        "test_dataloader = Dataset(test,input_size)\n",
        "test_dataloader = Dataloader(test_dataloader,batch_size = batch_size)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiex_ABvrjeS"
      },
      "source": [
        "## **CHeXNET Model (pretrained)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqV7xRCJrpfr"
      },
      "source": [
        "[CheXNET Model](https://arxiv.org/pdf/1711.05225.pdf) is a Denset121 layered model which is trained on 1000s of chest x-ray images for the classification of 14 diseases.We can load the weights of that model and pass the image through that model. The top layer will be ignore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QSD4DWKZtU"
      },
      "source": [
        "#chexnet weights ; https://drive.google.com/file/d/19BllaOvs2x5PLV_vlWMy4i8LapLb2j6b/view\n",
        "def create_chexnet(chexnet_weights = chexnet_weights):\n",
        "  \"\"\"\n",
        "  chexnet_weights: weights value in .h5 format of chexnet\n",
        "  creates a chexnet model with preloaded weights present in chexnet_weights file\n",
        "  \"\"\"\n",
        "  model = tf.keras.applications.DenseNet121(include_top=False) #importing densenet the last layer will be a relu activation layer\n",
        "\n",
        "  #we need to load the weights so setting the architecture of the model as same as the one of tha chexnet\n",
        "  x = model.output #output from chexnet\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x) #here activation is sigmoid as seen in research paper\n",
        "\n",
        "  chexnet = tf.keras.Model(inputs = model.input,outputs = x)\n",
        "  chexnet.load_weights(chexnet_weights)\n",
        "  chexnet = tf.keras.Model(inputs = model.input,outputs = chexnet.layers[-2].output)  #we will be taking the penultimate layer (second last layer here it is global avg maxpooling)\n",
        "  return chexnet"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMuXPuHmTi8R",
        "outputId": "4ebc2695-65e2-4d7e-ccfa-46b2356eb4c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# chexnet = create_chexnet()\n",
        "# chexnet.trainable = False\n",
        "\n",
        "# tf.keras.utils.plot_model(\n",
        "#     chexnet,\n",
        "#     to_file=\"model_1.png\",\n",
        "#     show_shapes=True,\n",
        "# )\n",
        "# del chexnet"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.692339 to fit\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFYNU3S1J0dA"
      },
      "source": [
        "class chexnet_layer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               name = \"chexnet_block\"\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.chexnet = create_chexnet()\n",
        "    self.chexnet.trainable = False\n",
        "  def call(self,data):\n",
        "    op = self.chexnet(data)\n",
        "    return op"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsam3CAGjJki",
        "outputId": "486a3dfd-3e33-44f6-e9f5-dead53555929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = tf.constant([1,2,4,5])\n",
        "tf.shape(a).numpy()[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x10sveDPIqLG"
      },
      "source": [
        "## **Encoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VRyWdh4o4YQ"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               chexnet = chexnet_layer(),\n",
        "               max_pad = max_pad,\n",
        "               embedding_dim = embedding_dim,\n",
        "               lstm_units = lstm_units,\n",
        "               vocab_size = vocab_size,\n",
        "               batch_size = batch_size,\n",
        "               dropout_rate = dropout_rate,\n",
        "               dense_dim = dense_dim\n",
        "               ):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.chexnet = chexnet\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.embedding = Embedding(input_dim  = vocab_size,\n",
        "                               output_dim = embedding_dim,\n",
        "                               input_length = max_pad,\n",
        "                               mask_zero = True,\n",
        "                               name = 'Encoder_Embedding'\n",
        "                                )\n",
        "    self.batch_norm = BatchNormalization()\n",
        "    self.globalavgpool = GlobalAveragePooling2D()\n",
        "    self.globalmaxpool = GlobalMaxPooling2D()\n",
        "    self.batch_size = batch_size\n",
        "    self.dense = Dense(256,\n",
        "                       activation = 'relu',\n",
        "                       name = 'Dense_Encoder'\n",
        "                       )\n",
        "    self.dropout = Dropout(dropout_rate)\n",
        "    self.concatenate = Concatenate()\n",
        "    self.input_length = max_pad\n",
        "\n",
        "\n",
        "  def call(self,dataset):\n",
        "    \"\"\"\n",
        "    takes the dataset from dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    self.image1 = dataset[0] #this is from x, y is not included here\n",
        "    self.image2 = dataset[1]\n",
        "    self.caption = dataset[2]\n",
        "\n",
        "    #getting backbone image features\n",
        "    image1 = self.chexnet(self.image1)\n",
        "    image2 = self.chexnet(self.image2)\n",
        "    concat = self.concatenate([image1,image2])\n",
        "    # gmaxpool = self.globalmaxpool(concat)\n",
        "    # gavgpool = self.globalavgpool(concat)\n",
        "    # concat = self.concatenate([gmaxpool,gavgpool])\n",
        "    concat = self.dropout(concat)\n",
        "    backbone_image_features = self.dense(concat)\n",
        "    backbone_image_features = tf.stack([backbone_image_features for i in range(self.input_length)],axis=-1) #introducing time axis, op_shape batch,dense_op,inp_length\n",
        "    #getting text embeddings\n",
        "    caption_features = self.embedding(self.caption) #op_shape : batch,inp_length,embedding size\n",
        "    shape = [self.batch_size,self.embedding_dim,self.input_length]\n",
        "    caption_features = tf.reshape(caption_features,shape) #op_shape : batch,embedding size ,inp_length\n",
        "    #concatenating these 3 features into one \n",
        "    backbone_features = Concatenate(axis=1)([backbone_image_features,caption_features]) #op_shape : batch,embedding size+desne_op ,inp_length\n",
        "\n",
        "    return backbone_features"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cDpL0H2KLe_"
      },
      "source": [
        "## Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7iCHGr7T2Sx"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lstm = LSTM(units = lstm_units\n",
        "                    )\n",
        "    self.fc1 = Dense(256,\n",
        "                    activation = 'relu',\n",
        "                    name = 'Decoder_FC'\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,backbone_features):\n",
        "    lstm_output = self.lstm(backbone_features)\n",
        "    fc1 = self.fc1(lstm_output)\n",
        "    return fc1"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPa1ewncvX1s"
      },
      "source": [
        "## Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lpZ9Ta0ymKu"
      },
      "source": [
        "class Encoder_Decoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               vocab_size = vocab_size\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder()\n",
        "    self.final = Dense(vocab_size,\n",
        "                      activation = 'softmax',\n",
        "                      name = 'Decoder_final_output_layer'\n",
        "                      )\n",
        "    \n",
        "  def call(self,dataset):\n",
        "    enc_op = self.encoder(dataset)\n",
        "    dec_op = self.decoder(enc_op)\n",
        "    final =  self.final(dec_op)\n",
        "\n",
        "    return final\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDYfu38T-XpJ"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = Encoder_Decoder()\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdUOzw8KHYMO",
        "outputId": "62615db7-2b58-483b-93af-648659044133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "model.fit(x = train_dataloader,\n",
        "          validation_data = test_dataloader,\n",
        "          epochs = 5\n",
        "          )"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-663e99c33b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(x = train_dataloader,\n\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m           )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[2,1462] labels_size=[2,73]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-129-663e99c33b6d>:3) ]] [Op:__inference_train_function_500450]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KtvBMEwwg-w"
      },
      "source": [
        "train_dataloader[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqrOKmmF8HMm"
      },
      "source": [
        "chexnet.predict(train_dataloader[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIf_U_408Xvq",
        "outputId": "9ec5bb76-b3cc-4613-aa4e-6e42fb556903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "enc = Encoder()(train_dataloader)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-f9ff3364ad71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-1a66bc89f4f8>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mimage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mgmaxpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mgavgpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgmaxpool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgavgpool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2618\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2619\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer global_max_pooling2d_14 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [64, 2048]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qVqXeLw868C",
        "outputId": "ab14ecc1-1717-4070-b2a5-5578ae1b3264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.constant([1,2,3])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JseSRDjx88Pa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}