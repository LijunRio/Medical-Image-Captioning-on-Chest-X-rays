{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Simple_encoder_decoder_Medical_Report.ipynb",
      "provenance": [],
      "mount_file_id": "1gMUkLlBDy2TJk-NpH7n4QchcvjYVFaT6",
      "authorship_tag": "ABX9TyOGQFa/Bq6qTCchz0oN6V6p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishthomaschempolil/Medical-Image-Captioning-on-Chest-X-rays/blob/main/2_Simple_encoder_decoder_Medical_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp78hvbnKJS5"
      },
      "source": [
        "image_folder = '/content/drive/My Drive/Medical image Reporting/Images'\n",
        "df_path = '/content/drive/My Drive/Medical image Reporting/df_final.pkl'\n",
        "chexnet_weights = '/content/drive/My Drive/Medical image Reporting/ChexNet weights/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HCj-KNqJ0MF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib #for saving model files as pkl\n",
        "import os\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import imgaug.augmenters as iaa\n",
        "sns.set(palette='muted',style='white')\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D, Input, Embedding, LSTM,Dot,Reshape,Concatenate,BatchNormalization, GlobalMaxPooling2D, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMKUtqyupMHb"
      },
      "source": [
        "# **Creating Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trX7yvkgpRYW",
        "outputId": "2993a9fd-2846-4a5e-e976-2e8cbf85b60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "df = pd.read_pickle(df_path)\n",
        "col = ['image_1','image_2','impression']\n",
        "df = df[col].copy()\n",
        "#path\n",
        "df['image_1'] = df['image_1'].apply(lambda row: os.path.join(image_folder,row)) #https://stackoverflow.com/a/61880790\n",
        "df['image_2'] = df['image_2'].apply(lambda row: os.path.join(image_folder,row))\n",
        "\n",
        "df.impression = '<CLS> ' + df.impression + ' <END>' \n",
        "print(df.shape)\n",
        "df.head(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4033, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_1</th>\n",
              "      <th>image_2</th>\n",
              "      <th>impression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>&lt;CLS&gt; bilateral lower lobe opacities . the app...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>/content/drive/My Drive/Medical image Reportin...</td>\n",
              "      <td>&lt;CLS&gt; bilateral lower lung airspace disease ri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             image_1  ...                                         impression\n",
              "0  /content/drive/My Drive/Medical image Reportin...  ...  <CLS> bilateral lower lobe opacities . the app...\n",
              "1  /content/drive/My Drive/Medical image Reportin...  ...  <CLS> bilateral lower lung airspace disease ri...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkRsmY0TuMah",
        "outputId": "838b47a7-4e1a-4767-aa19-e3228ea2b85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df.sample(random_state = 420,frac = 1).reset_index(drop = True) #shuffling the dataframe\n",
        "\n",
        "#splitting to train,test with 0.9,0.1 ratio\n",
        "train_size = int(0.9*df.shape[0])\n",
        "train = df[:train_size]\n",
        "test = df[train_size:].reset_index()\n",
        "\n",
        "del df,train_size\n",
        "train.shape,test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3629, 3), (404, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgjFwjEjpSFa",
        "outputId": "a080ba82-59c6-4c57-f2ce-681b7ab1d568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "source": [
        "#tokenizer\n",
        "tokenizer = Tokenizer(filters = '',oov_token = '<unk>') #setting filters to none\n",
        "tokenizer.fit_on_texts(train.impression.values)\n",
        "train_captions = tokenizer.texts_to_sequences(train.impression) \n",
        "test_captions = tokenizer.texts_to_sequences(test.impression) \n",
        "vocab_size = len(tokenizer.word_index)\n",
        "caption_len = np.array([len(i) for i in train_captions])\n",
        "start_index = tokenizer.word_index['<cls>'] #tokened value of <cls>\n",
        "end_index = tokenizer.word_index['<end>'] #tokened value of <end>\n",
        "\n",
        "#visualising impression length and other details\n",
        "ax = sns.displot(caption_len,height = 6)\n",
        "ax.set_titles('Value Counts vs Caption Length')\n",
        "ax.set_xlabels('Impresion length')\n",
        "plt.show()\n",
        "print('\\nValue Counts for caption length top 5 values\\n')\n",
        "print('Length|Counts')\n",
        "print(pd.Series(caption_len).value_counts()[:5])\n",
        "print('\\nThe max and min value of \"caption length\" was found to be %i and %i respectively'%(max(caption_len),min(caption_len)))\n",
        "print('The 99.5 percentile value of caption_len which is %i will be taken as the maximum padded value for each impression'%(np.percentile(caption_len,99.5)))\n",
        "max_pad = int(np.percentile(caption_len,99.5))\n",
        "del train_captions,test_captions #we will create tokenizing  and padding in-built in dataloader"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Value Counts for caption length top 5 values\n",
            "\n",
            "Length|Counts\n",
            "7     1111\n",
            "6      409\n",
            "8      342\n",
            "12     134\n",
            "9      122\n",
            "dtype: int64\n",
            "\n",
            "The max and min value of \"caption length\" was found to be 135 and 3 respectively\n",
            "The 99.5 percentile value of caption_len which is 74 will be taken as the maximum padded value for each impression\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8Y1fR8RIxWB"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju4j3gOoLBpJ"
      },
      "source": [
        "## **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WojEEVYsLErk"
      },
      "source": [
        "batch_size = 2\n",
        "embedding_dim = 300\n",
        "dense_dim = 256\n",
        "lstm_units = dense_dim+embedding_dim\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R58RYZR904HU"
      },
      "source": [
        "## **Creating an input data pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZWYibRUYWGu"
      },
      "source": [
        "class Dataset():\n",
        "  #here we will get the images converted to vector form and the corresponding captions\n",
        "  def __init__(self,df,input_size,tokenizer = tokenizer, augmentation = True,max_pad = max_pad): \n",
        "    \"\"\"\n",
        "    df  = dataframe containing image_1,image_2 and impression\n",
        "    \"\"\"\n",
        "    self.image1 = df.image_1\n",
        "    self.image2 = df.image_2\n",
        "    self.caption = df.impression\n",
        "    self.input_size = input_size #tuple ex: (512,512)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.augmentation = augmentation\n",
        "    self.max_pad = max_pad\n",
        "\n",
        "    #image augmentation\n",
        "    #https://imgaug.readthedocs.io/en/latest/source/overview/flip.html?highlight=Fliplr\n",
        "    self.aug1 = iaa.Fliplr(1) #flip images horizaontally\n",
        "    self.aug2 = iaa.Flipud(1) #flip images vertically\n",
        "\n",
        "    # https://imgaug.readthedocs.io/en/latest/source/overview/convolutional.html?highlight=emboss#emboss\n",
        "    self.aug3 = iaa.Emboss(alpha=(1), strength=1) #embosses image\n",
        "\n",
        "    #https://imgaug.readthedocs.io/en/latest/source/api_augmenters_convolutional.html?highlight=sharpen#imgaug.augmenters.convolutional.Sharpen\n",
        "    self.aug4 = iaa.Sharpen(alpha=(1.0), lightness=(1.5)) #sharpens the image and apply some lightness/brighteness 1 means fully sharpened etc\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    #gets the datapoint at i th index, we will extract the feature vectors of images after resizing the image to  and apply augmentation\n",
        "    #from image segmentation assignment\n",
        "    image1 = cv2.imread(self.image1[i],cv2.IMREAD_UNCHANGED)/255 \n",
        "    image2 = cv2.imread(self.image2[i],cv2.IMREAD_UNCHANGED)/255 #here there are 3 channels\n",
        "    image1 = cv2.resize(image1,self.input_size,interpolation = cv2.INTER_NEAREST)\n",
        "    image2 = cv2.resize(image2,self.input_size,interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "    #tokenizing and padding\n",
        "    caption = self.tokenizer.texts_to_sequences(self.caption[i:i+1]) #the input should be an array for tokenizer ie [self.caption[i]] \n",
        "\n",
        "    caption = pad_sequences(caption,maxlen = self.max_pad,padding = 'post') #opshape: batch_size*1*input_length\n",
        "    caption = tf.squeeze(caption,axis=0)\n",
        "\n",
        "\n",
        "    if self.augmentation: #we will not apply augmentation that crops the image \n",
        "          a = np.random.uniform()\n",
        "          if a<0.2:\n",
        "              image1 = self.aug1.augment_image(image1)\n",
        "              image2 = self.aug1.augment_image(image2)\n",
        "          elif a<0.4:\n",
        "              image1 = self.aug2.augment_image(image1)\n",
        "              image2 = self.aug2.augment_image(image2)\n",
        "          elif a<0.6:\n",
        "              image1 = self.aug3.augment_image(image1)\n",
        "              image2 = self.aug3.augment_image(image2)\n",
        "          elif a<0.8:\n",
        "              image1 = self.aug4.augment_image(image1)\n",
        "              image2 = self.aug4.augment_image(image2)\n",
        "          else: #applying no augmentation\n",
        "            pass;\n",
        "\n",
        "    \n",
        "    return image1,image2,caption\n",
        "\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.image1)\n",
        "\n",
        "\n",
        "class Dataloader(tf.keras.utils.Sequence):     #for batching\n",
        "    def __init__(self, dataset, batch_size=1, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.dataset[0]))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = [self.dataset[j] for j in range(start,stop)] #taken from Data class (calls __getitem__ of Data) here the shape is batch_size*3, (image_1,image_2,caption)\n",
        "        \n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)] #here the shape will become batch_size*input_size(of image)*3,batch_size*input_size(of image)*3\n",
        "                                                                      #,batch_size*1*max_pad\n",
        "\n",
        "\n",
        "        return tuple([[batch[0],batch[1],batch[2]],batch[2]]) #here [image1,image2, caption(without <END>)],caption(without <CLS>)\n",
        "    \n",
        "    def __len__(self): #returns total number of batches in an epoch\n",
        "        return len(self.indexes) // self.batch_size\n",
        "    \n",
        "    def on_epoch_end(self): #it runs at the end of epoch\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh3MLkj7ofJq"
      },
      "source": [
        "input_size = (224,224)\n",
        "train_dataloader = Dataset(train,input_size)\n",
        "train_dataloader = Dataloader(train_dataloader,batch_size = batch_size)\n",
        "\n",
        "test_dataloader = Dataset(test,input_size)\n",
        "test_dataloader = Dataloader(test_dataloader,batch_size = batch_size)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiex_ABvrjeS"
      },
      "source": [
        "## **CHeXNET Model (pretrained)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqV7xRCJrpfr"
      },
      "source": [
        "[CheXNET Model](https://arxiv.org/pdf/1711.05225.pdf) is a Denset121 layered model which is trained on 1000s of chest x-ray images for the classification of 14 diseases.We can load the weights of that model and pass the image through that model. The top layer will be ignore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QSD4DWKZtU"
      },
      "source": [
        "#chexnet weights ; https://drive.google.com/file/d/19BllaOvs2x5PLV_vlWMy4i8LapLb2j6b/view\n",
        "def create_chexnet(chexnet_weights = chexnet_weights):\n",
        "  \"\"\"\n",
        "  chexnet_weights: weights value in .h5 format of chexnet\n",
        "  creates a chexnet model with preloaded weights present in chexnet_weights file\n",
        "  \"\"\"\n",
        "  model = tf.keras.applications.DenseNet121(include_top=False) #importing densenet the last layer will be a relu activation layer\n",
        "\n",
        "  #we need to load the weights so setting the architecture of the model as same as the one of tha chexnet\n",
        "  x = model.output #output from chexnet\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x) #here activation is sigmoid as seen in research paper\n",
        "\n",
        "  chexnet = tf.keras.Model(inputs = model.input,outputs = x)\n",
        "  chexnet.load_weights(chexnet_weights)\n",
        "  chexnet = tf.keras.Model(inputs = model.input,outputs = chexnet.layers[-2].output)  #we will be taking the penultimate layer (second last layer here it is global avg maxpooling)\n",
        "  return chexnet"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMuXPuHmTi8R"
      },
      "source": [
        "# chexnet = create_chexnet()\n",
        "# chexnet.trainable = False\n",
        "\n",
        "# tf.keras.utils.plot_model(\n",
        "#     chexnet,\n",
        "#     to_file=\"model_1.png\",\n",
        "#     show_shapes=True,\n",
        "# )\n",
        "# del chexnet"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFYNU3S1J0dA"
      },
      "source": [
        "class chexnet_layer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               name = \"chexnet_block\"\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.chexnet = create_chexnet()\n",
        "    self.chexnet.trainable = False\n",
        "  def call(self,data):\n",
        "    op = self.chexnet(data)\n",
        "    return op"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x10sveDPIqLG"
      },
      "source": [
        "## **Encoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MxZ1rnAOW27"
      },
      "source": [
        "glove = {}\n",
        "with open('/content/drive/My Drive/glove_vectors/glove.6B.300d.txt',encoding='utf-8') as f: #taking 300 dimesions\n",
        "  for line in f:\n",
        "    word = line.split() #it is stored as string like this \"'the': '.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.4\"\n",
        "    glove[word[0]] = np.asarray(word[1:], dtype='float32')\n",
        "\n",
        "\n",
        "embedding_dim = 300\n",
        "# create a weight matrix for words in training docs for embedding purpose\n",
        "embedding_matrix = np.zeros((vocab_size+1, embedding_dim)) #https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  embedding_vector = glove.get(word)\n",
        "  if embedding_vector is not None: #if the word is found in glove vectors\n",
        "      embedding_matrix[i] = embedding_vector[:embedding_dim]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VRyWdh4o4YQ"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               chexnet = chexnet_layer(),\n",
        "               max_pad = max_pad,\n",
        "               embedding_dim = embedding_dim,\n",
        "               lstm_units = lstm_units,\n",
        "               vocab_size = vocab_size,\n",
        "               batch_size = batch_size,\n",
        "               dropout_rate = dropout_rate,\n",
        "               dense_dim = dense_dim\n",
        "               ):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.chexnet = chexnet\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.embedding = Embedding(input_dim  = vocab_size,\n",
        "                               output_dim = embedding_dim,\n",
        "                               input_length = max_pad,\n",
        "                               mask_zero = True,\n",
        "                               weights = [embedding_matrix],\n",
        "                               name = 'Encoder_Embedding'\n",
        "                                )\n",
        "    self.batch_norm = BatchNormalization()\n",
        "    self.globalavgpool = GlobalAveragePooling2D()\n",
        "    self.globalmaxpool = GlobalMaxPooling2D()\n",
        "    self.batch_size = batch_size\n",
        "    self.dense = Dense(256,\n",
        "                       activation = 'relu',\n",
        "                       name = 'Dense_Encoder'\n",
        "                       )\n",
        "    self.dropout = Dropout(dropout_rate)\n",
        "    self.concatenate = Concatenate()\n",
        "    self.input_length = max_pad\n",
        "\n",
        "\n",
        "  def call(self,dataset):\n",
        "    \"\"\"\n",
        "    takes the dataset from dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    self.image1 = dataset[0] #this is from x, y is not included here\n",
        "    self.image2 = dataset[1]\n",
        "    self.caption = dataset[2]\n",
        "\n",
        "    #getting backbone image features\n",
        "    image1 = self.chexnet(self.image1)\n",
        "    image2 = self.chexnet(self.image2)\n",
        "    concat = self.concatenate([image1,image2])\n",
        "    # gmaxpool = self.globalmaxpool(concat)\n",
        "    # gavgpool = self.globalavgpool(concat)\n",
        "    # concat = self.concatenate([gmaxpool,gavgpool])\n",
        "    concat = self.dropout(concat)\n",
        "    backbone_image_features = self.dense(concat)\n",
        "    backbone_image_features = tf.stack([backbone_image_features for i in range(self.input_length)],axis=-1) #introducing time axis, op_shape batch,dense_op,inp_length\n",
        "    #getting text embeddings\n",
        "    caption_features = self.embedding(self.caption) #op_shape : batch,inp_length,embedding size\n",
        "    shape = [self.batch_size,self.embedding_dim,self.input_length]\n",
        "    caption_features = tf.reshape(caption_features,shape) #op_shape : batch,embedding size ,inp_length\n",
        "    #concatenating these 3 features into one \n",
        "    backbone_features = Concatenate(axis=1)([backbone_image_features,caption_features]) #op_shape : batch,embedding size+desne_op ,inp_length\n",
        "\n",
        "    return backbone_features"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cDpL0H2KLe_"
      },
      "source": [
        "## Decoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7iCHGr7T2Sx"
      },
      "source": [
        "class One_Step_Decoder(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This layer outputs the softmaxed token\n",
        "  \"\"\"\n",
        "  def __init__(self,vocab_size = vocab_size):\n",
        "    super().__init__()\n",
        "    self.lstm = LSTM(units = lstm_units,\n",
        "                     return_sequences = True\n",
        "                    )\n",
        "    self.fc1 = Dense(256,\n",
        "                    activation = 'relu',\n",
        "                    name = 'One_step_Decoder_FC'\n",
        "                    )\n",
        "    self.final  = Dense(vocab_size,\n",
        "                    activation = 'softmax',\n",
        "                    name = 'One_step_Decoder_Final'\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,backbone_features):\n",
        "    lstm_output = self.lstm(backbone_features)\n",
        "    fc1 = self.fc1(lstm_output)\n",
        "    final = self.final(fc1)\n",
        "    return final"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPa1ewncvX1s"
      },
      "source": [
        "## Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lpZ9Ta0ymKu"
      },
      "source": [
        "class Encoder_Decoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               vocab_size = vocab_size\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = One_Step_Decoder()\n",
        "    self.final = Dense(vocab_size,\n",
        "                      activation = 'softmax',\n",
        "                      name = 'Decoder_final_output_layer'\n",
        "                      )\n",
        "    \n",
        "  def call(self,dataset):\n",
        "    enc_op = self.encoder(dataset)\n",
        "    for \n",
        "    dec_op = self.decoder(enc_op)\n",
        "    final =  self.final(dec_op)\n",
        "\n",
        "    return final\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDYfu38T-XpJ"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = Encoder_Decoder()\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdUOzw8KHYMO",
        "outputId": "5a06e6a8-20ac-42ff-e19d-3ed9f2410b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x = train_dataloader,\n",
        "          validation_data = test_dataloader,\n",
        "          epochs = 5\n",
        "          )"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-663e99c33b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(x = train_dataloader,\n\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m           )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, None) and (2, 556, 1462) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JseSRDjx88Pa",
        "outputId": "fa3cb274-8a29-47e1-f4c0-c12332295867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataloader[1][1].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 74)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hJtjhbQ-1ms",
        "outputId": "dcfaa700-9156-48b6-b2dd-39a4bc6d882b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataloader[1][1]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  5,  6, 75,  2,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 3,  5,  6,  7, 11,  2,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnpdoUNb_Hvc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}